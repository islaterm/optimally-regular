{
 "cells": [
  {
   "source": [
    "## Inicialización de Xavier/He\n",
    "\n",
    "En esta parte programarás la [inicialización de Xavier](http://proceedings.mlr.press/v9/glorot10a.html) que considera la inicialización de los pesos según el tamaño de cada capa. En particular para el tensor de parámetros $W^{(i)}$ de dimensiones $(d_{i-1},d_{i})$ (o sea, el que relaciona la salida de la capa $i-1$ y la entrada de la capa $i$), la inicialización de Xavier define cada parámetro $w$ en $W^{(i)}$ como\n",
    "\\begin{equation*}\n",
    "w := r \\sqrt{\\frac{1}{d_{i-1}}}\n",
    "\\end{equation*}\n",
    "donde $r$ es un número aleatorio con distribución normal unitaria (media 0 y varianza 1). Recuerda que para el caso en donde las funciones de activación de la capa $i$ sean `relu` la inicialización debiera llevar un $2$ (también conocida como [inicialización de He](https://arxiv.org/abs/1502.01852)) como se muestra a continuación:\n",
    "\\begin{equation*}\n",
    "w := r \\sqrt{\\frac{2}{d_{i-1}}}\n",
    "\\end{equation*}\n",
    "Cambia el inicializador de la clase de tu red neuronal agregándole un argumento opcional `init` con el que puedas decidir el tipo de inicialización de parámetros que quieras. En particular, si el valor del parámetros es el string `xavier` entonces se debe implementar la inicialización descrita (y si no se especifica nada, deberías usar la inicialización como lo habías hecho hasta la tarea pasada).\n",
    "\n",
    "### Observación\n",
    "\n",
    "Las dos formas de inicializar parámetros son *heurísticas* y tienen diversas variantes. Una variante estándar es si considerar una distribución normal (el valor de $r$ arriba) o una distribución uniforme. El [artículo original de X. Glorot & Y. Bengio](http://proceedings.mlr.press/v9/glorot10a.html) usa una distribución uniforme mientras el [artículo de K. He *et al.*](https://arxiv.org/abs/1502.01852) usa una normal. En el caso de `pytorch` se usa una [distribución uniforme](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L86). Si quieres puedes usar el mismo argumento `init` del inicializador para modificar el usar una distribución uniforme o normal (por ejemplo, usando el valor `xavier-uniform` para cambiar el comportamiento anterior). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}