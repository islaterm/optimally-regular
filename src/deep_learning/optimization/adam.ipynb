{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Adam\n",
    "\n",
    "En esta parte implementarás el algoritmo Adam (Adaptive Moments). Adam calcula un promedio exponencial móvil de los valores previos del gradiente y de los cuadrados del gradiente. El promedio de los gradientes lo usa para el paso del descenso del gradiente (como el SGD con momentum) y el promedio de los cuadrados de los gradientes para modificar la tasa de aprendizaje para cada parámetro por separado. Además Adam utiliza una corrección del sesgo inicial para considerar el hecho de que antes del entrenamiento todos los valores del gradiente (y el cuadrado) comienzan como 0. En específico, las actualizaciones de los parámetros según Adam están dadas por:\n",
    "<br>\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P_{\\partial \\theta} & := & \\beta_1P_{\\partial \\theta} + (1-\\beta_1)\\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\\\\n",
    "S_{\\partial \\theta} & := & \\beta_2 S_{\\partial \\theta} + (1-\\beta_2)\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\theta}*\\frac{\\partial \\mathcal{L}}{\\partial \\theta}\\right) \\\\\n",
    "\\overline{P_{\\partial \\theta}} & := & \\frac{P_{\\partial \\theta}}{1-{\\beta_1}^n} \\\\\n",
    "\\overline{S_{\\partial \\theta}} & := & \\frac{S_{\\partial \\theta}}{1-{\\beta_2}^n} \\\\\n",
    "\\theta & := & \\theta - \\lambda\\frac{1}{\\sqrt{\\overline{S_{\\partial \\theta}}}}*\\overline{P_{\\partial \\theta}}\n",
    "\\end{eqnarray*}\n",
    "<br>\n",
    "\n",
    "donde $P_{\\partial \\theta}$ y $S_{\\partial \\theta}$ son tensores de las mismas dimensiones que $\\theta$ y se inicializan como $0$ antes de comenzar el entrenamiento. Adicionalmente $n$ indica el paso de la iteración de descenso de gradiente, lo que se usa para corregir el sesgo inicial cuando se está computando $\\overline{P_{\\partial \\theta}}$ y $\\overline{S_{\\partial \\theta}}$ y la operación $*$ representa a una multiplicación punto a punto.\n",
    "\n",
    "Implementa una nueva clase `Adam` que implemente este optimizador. El inicializador debiera los parámetros `lr`, `beta1` y `beta2`, con valores por defecto 0.001, 0.9, y 0.999, respectivamente. Agrega un parámetro adicional `epsilon` con valor por defecto $10^{-8}$, y úsalo para evitar la división por $0$ sumándolo al denominador en la fórmula de arriba."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}