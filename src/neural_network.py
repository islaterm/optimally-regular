"""
"Back to training" (c) by Ignacio Slater M.
"Back to training" is licensed under a
Creative Commons Attribution 4.0 International License.
You should have received a copy of the license along with this
work. If not, see <http://creativecommons.org/licenses/by/4.0/>.
"""
from typing import Callable, List, Optional, Union

import torch
from torch import Tensor
from torch.nn import Parameter, ParameterList

from activation_functions import derivatives, softmax

ActivationFunction = Union[Callable[[torch.Tensor, float], Tensor], Callable[[Tensor], Tensor]]


def get_init_weights(shape):
    weights = torch.randn(shape)
    return Parameter(weights)


class FFNN(torch.nn.Module):
    __biases: ParameterList
    __weights: ParameterList
    __keep_prob: List[Tensor]
    __fn_args_mask: List[Optional[Parameter]]

    def __init__(self, features: int, hidden_layers: List[int],
                 activation_functions: List[ActivationFunction], classes: int,
                 ac_fn_arguments=None, keep_prob=None):
        super(FFNN, self).__init__()
        self.__setup_hyperparams(features, hidden_layers, classes)
        self.__setup_ac_funs(activation_functions, ac_fn_arguments)
        rand_matrix = [torch.rand_like(w) for w in self.__weights]
        self.__keep_prob = [] if keep_prob is None else [torch.ge(-1 * r, -1 * p) for r, p in
                                                         zip(rand_matrix, keep_prob)]
        self.__cache = []

    def __setup_hyperparams(self, features: int, hidden_layers: List[int], classes: int):
        sizes = [features] + hidden_layers + [classes]
        self.__weights = ParameterList(
            [get_init_weights((sizes[i], sizes[i + 1])) for i in range(len(sizes) - 1)])
        self.__biases = ParameterList([Parameter(torch.zeros(h)) for h in sizes[1:]])

    def __setup_ac_funs(self, activation_functions: List[ActivationFunction], ac_fn_arguments):
        self.__ac_functions = activation_functions
        self.__fn_args_mask = [Parameter(torch.tensor(p)) if p else None for p in ac_fn_arguments] \
            if ac_fn_arguments is not None else [None] * len(activation_functions)

    def forward(self, inputs: Tensor, predict: bool = False, device="cpu") -> Tensor:
        """
        Feeds the network with an input and returns the result of forward feeding it to the last
        layer.
        """
        self.__cache = []
        prediction = inputs
        if not predict:
            for weight, bias, ac_function, args, prob in zip(self.__weights[:-1],
                                                             self.__biases[:-1],
                                                             self.__ac_functions,
                                                             self.__fn_args_mask,
                                                             self.__keep_prob[:-1]):
                prob = prob.to(device)
                prediction = torch.mm(prediction, weight * prob) + bias
                self.__cache.append(prediction)
                prediction = ac_function(prediction, args.item()) if args else ac_function(
                    prediction)
            return softmax(
                torch.mm(prediction, self.__weights[-1] * self.__keep_prob[-1].to(device)) +
                self.__biases[-1], dim=1)
        for weight, bias, ac_function, args in zip(self.__weights[:-1], self.__biases[:-1],
                                                   self.__ac_functions, self.__fn_args_mask):
            prediction = torch.mm(prediction, weight) + bias
            self.__cache.append(prediction)
            prediction = ac_function(prediction, args.item()) if args else ac_function(
                prediction)
        return softmax(torch.mm(prediction, self.__weights[-1]) + self.__biases[-1], dim=1)

    def backward(self, inputs: Tensor, expected_output: Tensor, predicted_output: Tensor) -> None:
        """
        Performs a correction of the network parameters using a backpropagation algorithm.

        Args:
            inputs:
                the inputs fed to the network
            expected_output:
                the output expected of forward feeding the inputs to the network
            predicted_output:
                the actual output generated by the forward feeding
        """
        current_grad = (predicted_output - expected_output) / expected_output.size(0)

        for i in range(len(self.__weights) - 1, 0, -1):
            if self.__fn_args_mask[i - 1] is None:
                self.__weights[i].grad = self.__ac_functions[i - 1](
                    self.__cache[i - 1]).t() @ current_grad
            else:
                self.__weights[i].grad = \
                    self.__ac_functions[i - 1](self.__cache[i - 1],
                                               self.__fn_args_mask[i - 1].item()).t() @ current_grad
            self.__biases[i].grad = current_grad.sum(dim=0)
            h_grad = current_grad @ self.__weights[i].t()

            if self.__fn_args_mask[i - 1] is None:
                current_grad = derivatives[self.__ac_functions[i - 1]](self.__cache[i - 1]) * h_grad
            else:
                current_grad, p_grad = derivatives[self.__ac_functions[i - 1]](
                    self.__cache[i - 1], self.__fn_args_mask[i - 1])
                current_grad *= h_grad
                self.__fn_args_mask[i - 1].grad = (p_grad * h_grad).sum()

        self.__weights[0].grad = inputs.t() @ current_grad
        self.__biases[0].grad = current_grad.sum(dim=0)

    def load_weights(self, weights, out_weights, biases, out_bias):
        self.__weights = ParameterList([Parameter(W) for W in weights + [out_weights]])
        self.__biases = ParameterList([Parameter(b) for b in biases + [out_bias]])

    def num_parameters(self):
        total = 0
        for p in self.parameters():
            total += p.numel()
        return total
